#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import rclpy
from rclpy.node import Node
from rclpy.callback_groups import ReentrantCallbackGroup
from sensor_msgs.msg import Image
from std_msgs.msg import StringÂ  # --- ADDED: To handle the status message ---
from cv_bridge import CvBridge
import cv2
import numpy as np
from geometry_msgs.msg import TransformStamped, PointStamped, Quaternion
from tf2_ros import TransformBroadcaster, Buffer, TransformListener
from tf2_ros import LookupException, ConnectivityException, ExtrapolationException
import math

try:
Â  Â  from tf2_geometry_msgs import do_transform_point
except ImportError:
Â  Â  # Fallback/placeholder if tf2_geometry_msgs is not installed or import fails
Â  Â  def do_transform_point(point, transform):
Â  Â  Â  Â  raise NotImplementedError("tf2_geometry_msgs is required for do_transform_point")

SHOW_IMAGE = True
TEAM_ID = 2203Â 

# Marker IDs and Frame Names for ArUco
FERTILIZER_ARUCO_ID = 3Â 
FERTILIZER_BASE_ARUCO_ID = 6Â 
FRAME_NAMES = {
Â  Â  FERTILIZER_ARUCO_ID: f'{TEAM_ID}_fertiliser_1',Â  Â  Â # ID 3 is renamed to *_can
Â  Â  FERTILIZER_BASE_ARUCO_ID: f'{TEAM_ID}_ebot_base' # ID 6 is renamed to *_ebot_base
}

# Configuration for Robust Locking
CONFIDENCE_THRESHOLD = 5 # Number of consecutive detections required to lock the ArUco TF
POSITION_THRESHOLD = 0.10 # 10 cm proximity threshold for fruit matching

class VisionAndTFPublisher(Node):
Â  Â  """
Â  Â  ROS 2 Node to:
Â  Â  1. Detect ArUco markers, store their 'base_link' TF after a confidence threshold,
Â  Â  Â  Â and continuously broadcast the locked TF (original code 1 logic).
Â  Â  2. Detect 'bad fruits' using color/contour, find their 3D position using depthÂ 
Â  Â  Â  Â data, and continuously broadcast their TF (original code 2 logic).
Â  Â  """
Â  Â  def __init__(self):
Â  Â  Â  Â  super().__init__('vision_and_tf_publisher')
Â  Â  Â  Â  self.bridge = CvBridge()
Â  Â  Â  Â  self.cv_image = None
Â  Â  Â  Â  self.depth_image = NoneÂ 
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- ADDED: Flag to control execution ---
Â  Â  Â  Â  self.is_detection_enabled = FalseÂ 

Â  Â  Â  Â  # Camera Intrinsics (Hardcoded values are typically loaded from a sensor_msgs/CameraInfo topic)
Â  Â  Â  Â  self.centerCamX = 642.724365234375
Â  Â  Â  Â  self.centerCamY = 361.9780578613281
Â  Â  Â  Â  self.focalX = 915.3003540039062
Â  Â  Â  Â  self.focalY = 914.0320434570312
Â  Â  Â  Â  self.camera_matrix = np.array([[self.focalX, 0, self.centerCamX], [0, self.focalY, self.centerCamY], [0, 0, 1]])
Â  Â  Â  Â  self.dist_coeffs = np.zeros((4, 1))
Â  Â  Â  Â  self.marker_size = 0.13 # Using 0.13 from ArUco code
Â  Â  Â  Â Â 
Â  Â  Â  Â  # ArUco Setup
Â  Â  Â  Â  self.aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_50)
Â  Â  Â  Â  self.aruco_params = cv2.aruco.DetectorParameters()
Â  Â  Â  Â  self.aruco_params.adaptiveThreshWinSizeMin = 3
Â  Â  Â  Â  self.aruco_params.adaptiveThreshWinSizeMax = 23
Â  Â  Â  Â  self.aruco_params.minMarkerPerimeterRate = 0.03
Â  Â  Â  Â  self.aruco_params.maxMarkerPerimeterRate = 4.0
Â  Â  Â  Â Â 
Â  Â  Â  Â  self.tf_broadcaster = TransformBroadcaster(self)
Â  Â  Â  Â  self.tf_buffer = Buffer()
Â  Â  Â  Â  self.tf_listener = TransformListener(self.tf_buffer, self)
Â  Â  Â  Â  self.cb_group = ReentrantCallbackGroup()Â 

Â  Â  Â  Â  # TF Locking Mechanism (for ArUco)
Â  Â  Â  Â  self.locked_tfs = {id: None for id in FRAME_NAMES.keys()}
Â  Â  Â  Â  self.detection_confidence_counter = {id: 0 for id in FRAME_NAMES.keys()}
Â  Â  Â  Â Â 
Â  Â  Â  Â  # TF Locking Mechanism (for Bad Fruits)
Â  Â  Â  Â  self.locked_fruit_tfs = {}Â  # {fruit_id: TransformStamped}
Â  Â  Â  Â  self.fruit_detection_confidence = {}Â  # {fruit_id: counter}
Â  Â  Â  Â  self.fruit_detection_positions = {}Â  # {fruit_id: [(x, y, z), ...]} - Stores 'camera_link' positions pre-lock
Â  Â  Â  Â Â 
Â  Â  Â  Â  # SubscriptionsÂ 
Â  Â  Â  Â  self.create_subscription(
Â  Â  Â  Â  Â  Â  Image, '/camera/image_raw', self.colorimagecb, 10, callback_group=self.cb_group
Â  Â  Â  Â  )
Â  Â  Â  Â  self.create_subscription(
Â  Â  Â  Â  Â  Â  Image, '/camera/depth/image_raw', self.depthimagecb, 10, callback_group=self.cb_group
Â  Â  Â  Â  )

Â  Â  Â  Â  # --- ADDED: Subscription for status check ---
Â  Â  Â  Â  self.create_subscription(
Â  Â  Â  Â  Â  Â  String, '/detection_status', self.detection_status_cb, 10, callback_group=self.cb_group
Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Main loop timerÂ 
Â  Â  Â  Â  self.create_timer(0.1, self.process_vision_and_tf, callback_group=self.cb_group)

Â  Â  Â  Â  if SHOW_IMAGE:
Â  Â  Â  Â  Â  Â  cv2.namedWindow('Vision and TF Publisher', cv2.WINDOW_NORMAL)
Â  Â  Â  Â  Â  Â  cv2.resizeWindow('Vision and TF Publisher', 1280, 720)
Â  Â  Â  Â Â 
Â  Â  Â  Â  self.get_logger().info(f"VisionAndTFPublisher node started. Team ID: {TEAM_ID}")
Â  Â  Â  Â  self.get_logger().info("Waiting for /detection_status trigger...")

Â  Â  ## --- CALLBACKS ---
Â  Â Â 
Â  Â  # --- ADDED: Callback to enable vision logic ---
Â  Â  def detection_status_cb(self, msg):
Â  Â  Â  Â  """Enable vision only when 'DOCK_STATION' is received."""
Â  Â  Â  Â  if "DOCK_STATION" in msg.data:
Â  Â  Â  Â  Â  Â  if not self.is_detection_enabled:
Â  Â  Â  Â  Â  Â  Â  Â  self.is_detection_enabled = True
Â  Â  Â  Â  Â  Â  Â  Â  self.get_logger().info(f"Received Trigger: {msg.data}. Vision System ENABLED.")

Â  Â  def colorimagecb(self, data):
Â  Â  Â  Â  """Callback function for color image topic."""
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  self.cv_image = self.bridge.imgmsg_to_cv2(data, desired_encoding='bgr8')
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  self.get_logger().error(f'Error converting color image: {e}')

Â  Â  def depthimagecb(self, data):
Â  Â  Â  Â  """Callback function for depth image topic (converts 16UC1 to meters)."""
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  depth_img = self.bridge.imgmsg_to_cv2(data, desired_encoding='passthrough')
Â  Â  Â  Â  Â  Â  if depth_img.dtype == np.uint16:
Â  Â  Â  Â  Â  Â  Â  Â  self.depth_image = depth_img.astype(np.float32) / 1000.0Â 
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  self.depth_image = depth_img
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  self.get_logger().error(f'Error converting depth image: {e}')
Â  Â Â 
Â  Â  # --- HELPER METHODS ---
Â  Â Â 
Â  Â  def rvec_tvec_to_transform(self, rvec, tvec):
Â  Â  Â  Â  """Converts rvec/tvec to a TransformStamped message (optical frame). (UNMODIFIED)"""
Â  Â  Â  Â  rotation_matrix, _ = cv2.Rodrigues(rvec)
Â  Â  Â  Â  q = Quaternion()
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Quaternion Calculation logic from original code 1
Â  Â  Â  Â  trace = rotation_matrix[0, 0] + rotation_matrix[1, 1] + rotation_matrix[2, 2]
Â  Â  Â  Â  if trace > 0:
Â  Â  Â  Â  Â  Â  s = math.sqrt(trace + 1.0) * 2
Â  Â  Â  Â  Â  Â  q.w = 0.25 * s
Â  Â  Â  Â  Â  Â  q.x = (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / s
Â  Â  Â  Â  Â  Â  q.y = (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / s
Â  Â  Â  Â  Â  Â  q.z = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  s = math.sqrt(1.0 + rotation_matrix[2, 2] - rotation_matrix[0, 0] - rotation_matrix[1, 1]) * 2
Â  Â  Â  Â  Â  Â  q.w = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s
Â  Â  Â  Â  Â  Â  q.x = (rotation_matrix[0, 2] + rotation_matrix[2, 0]) / s
Â  Â  Â  Â  Â  Â  q.y = (rotation_matrix[1, 2] + rotation_matrix[2, 1]) / s
Â  Â  Â  Â  Â  Â  q.z = 0.25 * s
Â  Â  Â  Â Â 
Â  Â  Â  Â  transform = TransformStamped()
Â  Â  Â  Â  transform.transform.translation.x = float(tvec[0][0])
Â  Â  Â  Â  transform.transform.translation.y = float(tvec[0][1])
Â  Â  Â  Â  transform.transform.translation.z = float(tvec[0][2])
Â  Â  Â  Â  transform.transform.rotation = q
Â  Â  Â  Â  return transform

Â  Â  def detect_aruco_markers(self, image):
Â  Â  Â  Â  """Detects ArUco markers and estimates their pose. (UNMODIFIED)"""
Â  Â  Â  Â  detected_markers = []
Â  Â  Â  Â  if image is None: return detected_markers
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
Â  Â  Â  Â  Â  Â  corners, ids, _ = cv2.aruco.detectMarkers(gray, self.aruco_dict, parameters=self.aruco_params)
Â  Â  Â  Â  Â  Â  if ids is None or len(ids) == 0: return detected_markers
Â  Â  Â  Â  Â  Â  rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(
Â  Â  Â  Â  Â  Â  Â  Â  corners, self.marker_size, self.camera_matrix, self.dist_coeffs
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  for i in range(len(ids)):
Â  Â  Â  Â  Â  Â  Â  Â  detected_markers.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'id': int(ids[i][0]),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'corners': corners[i],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'rvec': rvecs[i],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'tvec': tvecs[i]
Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  self.get_logger().error(f'Error in ArUco detection: {e}')
Â  Â  Â  Â  return detected_markers
Â  Â  Â  Â Â 
Â  Â  def draw_axes(self, image, rvec, tvec, length=0.07):
Â  Â  Â  Â  """Draws the 3D axes on the image for visualization. (UNMODIFIED)"""
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  cv2.drawFrameAxes(
Â  Â  Â  Â  Â  Â  Â  Â  image, self.camera_matrix, self.dist_coeffs, rvec, tvec, length, thickness=6
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  self.get_logger().error(f'Error drawing axes: {e}')Â 
Â  Â  Â  Â  Â  Â Â 
Â  Â  def get_depth_at_point(self, x, y):
Â  Â  Â  Â  """Retrieves the depth value (in meters) at a specific pixel (x, y). (UNMODIFIED)"""
Â  Â  Â  Â  if self.depth_image is None:
Â  Â  Â  Â  Â  Â  return None
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  height, width = self.depth_image.shape
Â  Â  Â  Â  Â  Â  x = max(0, min(x, width - 1))
Â  Â  Â  Â  Â  Â  y = max(0, min(y, height - 1))
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  depth_value = self.depth_image[y, x]

Â  Â  Â  Â  Â  Â  if depth_value <= 0.0 or np.isnan(depth_value):
Â  Â  Â  Â  Â  Â  Â  Â  return None
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  return float(depth_value)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  self.get_logger().error(f'Error getting depth: {e}')
Â  Â  Â  Â  Â  Â  return None
Â  Â  Â  Â  Â  Â Â 
Â  Â  def compute_3d_position(self, pixel_x, pixel_y, depth):
Â  Â  Â  Â  """Converts 2D pixel coordinates and depth to 3D in 'camera_link' frame. (UNMODIFIED)"""
Â  Â  Â  Â  z_depth = float(depth)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 1. Standard Pinhole Model (Optical Frame: X_opt=Right, Y_opt=Down, Z_opt=Depth)
Â  Â  Â  Â  x_optical = z_depth * (pixel_x - self.centerCamX) / self.focalX
Â  Â  Â  Â  y_optical = z_depth * (pixel_y - self.centerCamY) / self.focalY
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 2. Map Optical to Link Frame (Standard ROS camera convention)
Â  Â  Â  Â  pos_x_cam = z_depthÂ  Â  # X_link = Z_optical (Depth)
Â  Â  Â  Â  pos_y_cam = -x_optical # Y_link = -X_optical (Invert Right to get Left)
Â  Â  Â  Â  pos_z_cam = -y_optical # Z_link = -Y_optical (Invert Down to get Up)
Â  Â  Â  Â Â 
Â  Â  Â  Â  return pos_x_cam, pos_y_cam, pos_z_cam

Â  Â  def create_fruit_mask(self, image):
Â  Â  Â  Â  """Creates an ROI mask focused on the conveyor belt/tray. (UNMODIFIED)"""
Â  Â  Â  Â  height, width = image.shape[:2]
Â  Â  Â  Â  mask = np.zeros((height, width), dtype=np.uint8)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Define a general ROI for the area of interest
Â  Â  Â  Â  tray_x_start = 50
Â  Â  Â  Â  tray_x_end = width // 2
Â  Â  Â  Â  tray_y_start = height // 4
Â  Â  Â  Â  tray_y_end = 3 * height // 4
Â  Â  Â  Â Â 
Â  Â  Â  Â  mask[tray_y_start:tray_y_end, tray_x_start:tray_x_end] = 255
Â  Â  Â  Â Â 
Â  Â  Â  Â  return mask

Â  Â  def bad_fruit_detection(self, rgb_image):
Â  Â  Â  Â  """Detects bad fruits using a robust color mask and contour detection. (UNMODIFIED)"""
Â  Â  Â  Â  bad_fruits = []
Â  Â  Â  Â  if rgb_image is None:
Â  Â  Â  Â  Â  Â  return bad_fruits

Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  tray_mask = self.create_fruit_mask(rgb_image)
Â  Â  Â  Â  Â  Â  hsv_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2HSV)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Color masking for 'bad' colors (brown/grey/dark)
Â  Â  Â  Â  Â  Â  brown_mask = cv2.inRange(hsv_image, np.array([5, 50, 20]), np.array([25, 255, 100]))
Â  Â  Â  Â  Â  Â  grey_mask = cv2.inRange(hsv_image, np.array([0, 0, 100]), np.array([180, 50, 255]))
Â  Â  Â  Â  Â  Â  dark_mask = cv2.inRange(hsv_image, np.array([0, 0, 0]), np.array([180, 255, 60]))
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  combined_mask = cv2.bitwise_or(brown_mask, grey_mask)
Â  Â  Â  Â  Â  Â  combined_mask = cv2.bitwise_or(combined_mask, dark_mask)
Â  Â  Â  Â  Â  Â  final_mask = cv2.bitwise_and(combined_mask, tray_mask)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Morphological operations for noise reduction
Â  Â  Â  Â  Â  Â  kernel = np.ones((5, 5), np.uint8)
Â  Â  Â  Â  Â  Â  final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_CLOSE, kernel)
Â  Â  Â  Â  Â  Â  final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_OPEN, kernel)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  contours, _ = cv2.findContours(final_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  fruit_id = 1
Â  Â  Â  Â  Â  Â  for contour in contours:
Â  Â  Â  Â  Â  Â  Â  Â  area = cv2.contourArea(contour)
Â  Â  Â  Â  Â  Â  Â  Â  # Area and aspect ratio filtering
Â  Â  Â  Â  Â  Â  Â  Â  if 800 < area < 25000:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  x, y, w, h = cv2.boundingRect(contour)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  aspect_ratio = w / h if h > 0 else 0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if 0.5 < aspect_ratio < 2.0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  center_x = x + w // 2
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  center_y = y + h // 2
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Get depth for the fruit center
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  depth = self.get_depth_at_point(center_x, center_y)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if depth is None or depth == 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Fallback depth if sensor data is bad
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  depth = 0.5Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fruit_info = {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'id': fruit_id, # This ID is only local to this frame, will be matched/reassigned later
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'center': (center_x, center_y),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'depth': depth,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'bbox': (x, y, w, h),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'contour': contour,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  }
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  bad_fruits.append(fruit_info)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fruit_id += 1
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  self.get_logger().error(f'Error in bad fruit detection: {e}')
Â  Â  Â  Â Â 
Â  Â  Â  Â  return bad_fruits

Â  Â  # --- TF LOCKING HELPER METHODS ---
Â  Â Â 
Â  Â  # ðŸŒŸ CORRECTED METHOD: Must check against both pre-locked and locked TFs.
Â  Â  def match_fruit_to_tracked(self, pixel_x, pixel_y, depth):
Â  Â  Â  Â  """
Â  Â  Â  Â  Matches a detected fruit to an existing tracked (pre-lock) or locked fruitÂ 
Â  Â  Â  Â  based on 3D position proximity in the 'camera_link' frame.
Â  Â  Â  Â  """
Â  Â  Â  Â  # Compute 3D position of the current detection in 'camera_link' frame
Â  Â  Â  Â  pos_x, pos_y, pos_z = self.compute_3d_position(pixel_x, pixel_y, depth)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # A. Check against currently tracked (pre-locked) fruits
Â  Â  Â  Â  for fruit_id, positions in self.fruit_detection_positions.items():
Â  Â  Â  Â  Â  Â  if positions:
Â  Â  Â  Â  Â  Â  Â  Â  # Use the last known position for comparison
Â  Â  Â  Â  Â  Â  Â  Â  last_pos = positions[-1]Â 
Â  Â  Â  Â  Â  Â  Â  Â  distance = math.sqrt(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (pos_x - last_pos[0])**2 +Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (pos_y - last_pos[1])**2 +Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  (pos_z - last_pos[2])**2
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if distance < POSITION_THRESHOLD:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return fruit_idÂ  # Match found (pre-locked)

Â  Â  Â  Â  # B. Check against currently locked fruits
Â  Â  Â  Â  for fruit_id, tf_msg in self.locked_fruit_tfs.items():
Â  Â  Â  Â  Â  Â  # If already locked, its TF is in 'base_link'. We must transform it backÂ 
Â  Â  Â  Â  Â  Â  # to 'camera_link' for comparison with the new detection.
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  base_pt = PointStamped()
Â  Â  Â  Â  Â  Â  Â  Â  base_pt.header.frame_id = tf_msg.header.frame_id # 'base_link'
Â  Â  Â  Â  Â  Â  Â  Â  # Use Time(0) to get the latest available transform
Â  Â  Â  Â  Â  Â  Â  Â  base_pt.header.stamp = rclpy.time.Time().to_msg()Â 
Â  Â  Â  Â  Â  Â  Â  Â  base_pt.point.x = tf_msg.transform.translation.x
Â  Â  Â  Â  Â  Â  Â  Â  base_pt.point.y = tf_msg.transform.translation.y
Â  Â  Â  Â  Â  Â  Â  Â  base_pt.point.z = tf_msg.transform.translation.z
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Lookup transform from 'base_link' to 'camera_link'
Â  Â  Â  Â  Â  Â  Â  Â  base_to_camera = self.tf_buffer.lookup_transform(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'camera_link', 'base_link', rclpy.time.Time()
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Transform the locked point to 'camera_link'
Â  Â  Â  Â  Â  Â  Â  Â  cam_pt_locked = do_transform_point(base_pt, base_to_camera)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Compare distance in 'camera_link' frame
Â  Â  Â  Â  Â  Â  Â  Â  dist_x = pos_x - cam_pt_locked.point.x
Â  Â  Â  Â  Â  Â  Â  Â  dist_y = pos_y - cam_pt_locked.point.y
Â  Â  Â  Â  Â  Â  Â  Â  dist_z = pos_z - cam_pt_locked.point.z
Â  Â  Â  Â  Â  Â  Â  Â  distance = math.sqrt(dist_x**2 + dist_y**2 + dist_z**2)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if distance < POSITION_THRESHOLD:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return fruit_id # Match found (locked)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  except (LookupException, ConnectivityException, ExtrapolationException) as e:
Â  Â  Â  Â  Â  Â  Â  Â  # Silently fail if TF lookup is temporarily unavailable
Â  Â  Â  Â  Â  Â  Â  Â  pass
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  self.get_logger().error(f'Error during locked fruit matching: {e}')
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  return NoneÂ  # New fruit
Â  Â Â 
Â  Â  def lock_fruit_tf(self, fruit_id, pos_x_cam, pos_y_cam, pos_z_cam):
Â  Â  Â  Â  """Locks a bad fruit's TF in base_link frame after confidence threshold. (UNMODIFIED)"""
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  current_stamp = self.get_clock().now().to_msg()
Â  Â  Â  Â  Â  Â  frame_name = f'{TEAM_ID}_bad_fruit_{fruit_id}'
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Transform to base_link
Â  Â  Â  Â  Â  Â  cam_pt = PointStamped()
Â  Â  Â  Â  Â  Â  cam_pt.header.frame_id = 'camera_link'
Â  Â  Â  Â  Â  Â  cam_pt.header.stamp = current_stamp
Â  Â  Â  Â  Â  Â  cam_pt.point.x = pos_x_cam
Â  Â  Â  Â  Â  Â  cam_pt.point.y = pos_y_cam
Â  Â  Â  Â  Â  Â  cam_pt.point.z = pos_z_cam
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Lookup transform (rclpy.time.Time() for latest available)
Â  Â  Â  Â  Â  Â  camera_to_base = self.tf_buffer.lookup_transform('base_link', 'camera_link', rclpy.time.Time())
Â  Â  Â  Â  Â  Â  base_pt = do_transform_point(cam_pt, camera_to_base)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Create and store the locked TF
Â  Â  Â  Â  Â  Â  tf_msg = TransformStamped()
Â  Â  Â  Â  Â  Â  tf_msg.header.frame_id = 'base_link'
Â  Â  Â  Â  Â  Â  tf_msg.child_frame_id = frame_name
Â  Â  Â  Â  Â  Â  tf_msg.transform.translation.x = base_pt.point.x
Â  Â  Â  Â  Â  Â  tf_msg.transform.translation.y = base_pt.point.y
Â  Â  Â  Â  Â  Â  tf_msg.transform.translation.z = base_pt.point.z
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Orientation for downward-pointing gripper
Â  Â  Â  Â  Â  Â  tf_msg.transform.rotation.x = 0.0
Â  Â  Â  Â  Â  Â  tf_msg.transform.rotation.y = 1.0
Â  Â  Â  Â  Â  Â  tf_msg.transform.rotation.z = 0.0
Â  Â  Â  Â  Â  Â  tf_msg.transform.rotation.w = 0.0
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  self.locked_fruit_tfs[fruit_id] = tf_msg
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  self.get_logger().info(
Â  Â  Â  Â  Â  Â  Â  Â  f'âœ… LOCKED {frame_name.upper()} after {CONFIDENCE_THRESHOLD} detections '
Â  Â  Â  Â  Â  Â  Â  Â  f'at X:{base_pt.point.x:.3f}, Y:{base_pt.point.y:.3f}, Z:{base_pt.point.z:.3f}'
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  except (LookupException, ConnectivityException, ExtrapolationException) as e:
Â  Â  Â  Â  Â  Â  self.get_logger().warn(f'Waiting for base_link TF to lock fruit {fruit_id}: {e}', throttle_duration_sec=1.0)
Â  Â  Â  Â  Â  Â  self.fruit_detection_confidence[fruit_id] = 0
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  self.get_logger().error(f'Error locking fruit TF for fruit_{fruit_id}: {e}')
Â  Â  Â  Â  Â  Â  self.fruit_detection_confidence[fruit_id] = 0

Â  Â  # --- TF LOCKING (for ArUco) ---
Â  Â  def lock_marker_tf(self, marker):
Â  Â  Â  Â  """Calculates and stores the ArUco marker's TF in the base_link frame. (UNMODIFIED)"""
Â  Â  Â  Â  marker_id = marker['id']
Â  Â  Â  Â  rvec = marker['rvec']
Â  Â  Â  Â  tvec = marker['tvec']
Â  Â  Â  Â  frame_name = FRAME_NAMES[marker_id]
Â  Â  Â  Â Â 
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  current_stamp = self.get_clock().now().to_msg()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 1. Convert translation from Optical (ArUco output) to Link FrameÂ 
Â  Â  Â  Â  Â  Â  aruco_tf_optical = self.rvec_tvec_to_transform(rvec, tvec)
Â  Â  Â  Â  Â  Â  x_opt = aruco_tf_optical.transform.translation.x
Â  Â  Â  Â  Â  Â  y_opt = aruco_tf_optical.transform.translation.y
Â  Â  Â  Â  Â  Â  z_opt = aruco_tf_optical.transform.translation.z
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Optical to Link Frame (ROS convention: X=Fwd, Y=Lft, Z=Up)
Â  Â  Â  Â  Â  Â  x_link = z_optÂ Â 
Â  Â  Â  Â  Â  Â  y_link = -x_optÂ 
Â  Â  Â  Â  Â  Â  z_link = -y_optÂ 
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 2. Transform position to 'base_link'
Â  Â  Â  Â  Â  Â  cam_pt = PointStamped()
Â  Â  Â  Â  Â  Â  cam_pt.header.frame_id = 'camera_link'
Â  Â  Â  Â  Â  Â  cam_pt.header.stamp = current_stamp
Â  Â  Â  Â  Â  Â  cam_pt.point.x, cam_pt.point.y, cam_pt.point.z = x_link, y_link, z_link

Â  Â  Â  Â  Â  Â  # Lookup transform (rclpy.time.Time() for latest available)
Â  Â  Â  Â  Â  Â  camera_to_base = self.tf_buffer.lookup_transform('base_link', 'camera_link', rclpy.time.Time())
Â  Â  Â  Â  Â  Â  base_pt = do_transform_point(cam_pt, camera_to_base)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 3. Define Fixed Rotation (to match the object's expected frame)
Â  Â  Â  Â  Â  Â  q_fixed = Quaternion()
Â  Â  Â  Â  Â  Â  if marker_id == FERTILIZER_ARUCO_ID:
Â  Â  Â  Â  Â  Â  Â  Â  q_fixed = Quaternion(x=0.5, y=0.5, z=-0.5, w=0.5)Â 
Â  Â  Â  Â  Â  Â  elif marker_id == FERTILIZER_BASE_ARUCO_ID:
Â  Â  Â  Â  Â  Â  Â  Â  q_fixed = Quaternion(x=1.0, y=0.0, z=0.0, w=0.0)Â 
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 4. Create and Store the TransformStamped object
Â  Â  Â  Â  Â  Â  tf_msg_base = TransformStamped()
Â  Â  Â  Â  Â  Â  tf_msg_base.header.frame_id = 'base_link'
Â  Â  Â  Â  Â  Â  tf_msg_base.child_frame_id = frame_name
Â  Â  Â  Â  Â  Â  tf_msg_base.transform.translation.x = base_pt.point.x
Â  Â  Â  Â  Â  Â  tf_msg_base.transform.translation.y = base_pt.point.y
Â  Â  Â  Â  Â  Â  tf_msg_base.transform.translation.z = base_pt.point.zÂ 
Â  Â  Â  Â  Â  Â  tf_msg_base.transform.rotation = q_fixed
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  self.locked_tfs[marker_id] = tf_msg_base
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  self.get_logger().info(
Â  Â  Â  Â  Â  Â  Â  Â  f'âœ… LOCKED {frame_name.upper()} (ID: {marker_id}) after {CONFIDENCE_THRESHOLD} detections '
Â  Â  Â  Â  Â  Â  Â  Â  f'at X:{base_pt.point.x:.3f}, Y:{base_pt.point.y:.3f}, Z:{base_pt.point.z:.3f}'
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  except (LookupException, ConnectivityException, ExtrapolationException) as e:
Â  Â  Â  Â  Â  Â  self.get_logger().warn(f'Waiting for base_link TF to lock {frame_name} (ID {marker_id}): {e}', throttle_duration_sec=1.0)
Â  Â  Â  Â  Â  Â  self.detection_confidence_counter[marker_id] = 0 # Reset counter if TF lookup fails
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  self.get_logger().error(f'Error locking ArUco TF for {frame_name}: {e}')
Â  Â  Â  Â  Â  Â  self.detection_confidence_counter[marker_id] = 0 # Reset counter on error

Â  Â  # --- MAIN PROCESS LOOP ---
Â  Â  def process_vision_and_tf(self):
Â  Â  Â  Â  # --- Check flag before running logic ---
Â  Â  Â  Â  if not self.is_detection_enabled:
Â  Â  Â  Â  Â  Â  return

Â  Â  Â  Â  if self.cv_image is None:
Â  Â  Â  Â  Â  Â  return

Â  Â  Â  Â  current_stamp = self.get_clock().now().to_msg()
Â  Â  Â  Â  display_image = self.cv_image.copy()
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- 1. ArUco Detection and Locking (Original Code 1 Logic) ---
Â  Â  Â  Â  detected_ids_in_frame = set()
Â  Â  Â  Â  aruco_markers = self.detect_aruco_markers(self.cv_image)
Â  Â  Â  Â Â 
Â  Â  Â  Â  for marker in aruco_markers:
Â  Â  Â  Â  Â  Â  marker_id = marker['id']
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if marker_id not in FRAME_NAMES:
Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  detected_ids_in_frame.add(marker_id)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Visualization
Â  Â  Â  Â  Â  Â  if display_image is not None:
Â  Â  Â  Â  Â  Â  Â  Â  cv2.aruco.drawDetectedMarkers(display_image, [marker['corners']], borderColor=(0, 255, 255))
Â  Â  Â  Â  Â  Â  Â  Â  self.draw_axes(display_image, marker['rvec'], marker['tvec'], length=0.07)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Locking Logic
Â  Â  Â  Â  Â  Â  if self.locked_tfs[marker_id] is None:
Â  Â  Â  Â  Â  Â  Â  Â  self.detection_confidence_counter[marker_id] = min(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  CONFIDENCE_THRESHOLD, self.detection_confidence_counter[marker_id] + 1
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if self.detection_confidence_counter[marker_id] >= CONFIDENCE_THRESHOLD:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.lock_marker_tf(marker)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Reset confidence for non-detected, unlocked markers
Â  Â  Â  Â  for marker_id in self.locked_tfs:
Â  Â  Â  Â  Â  Â  if self.locked_tfs[marker_id] is None and marker_id not in detected_ids_in_frame:
Â  Â  Â  Â  Â  Â  Â  Â  self.detection_confidence_counter[marker_id] = 0
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  # Continuous Broadcasting of Locked ArUco TFs
Â  Â  Â  Â  for marker_id, tf_msg in self.locked_tfs.items():
Â  Â  Â  Â  Â  Â  if tf_msg is not None:
Â  Â  Â  Â  Â  Â  Â  Â  tf_msg.header.stamp = current_stamp
Â  Â  Â  Â  Â  Â  Â  Â  self.tf_broadcaster.sendTransform(tf_msg)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- 2. Bad Fruit Detection and TF Locking (With Confidence Mechanism) ---
Â  Â  Â  Â Â 
Â  Â  Â  Â  # MODIFIED: Stop bad fruit detection if ANY fruit is already locked.
Â  Â  Â  Â  if len(self.locked_fruit_tfs) == 0:
Â  Â  Â  Â  Â  Â  bad_fruits = self.bad_fruit_detection(self.cv_image)
Â  Â  Â  Â  Â  Â  detected_fruit_ids_in_frame = set()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  for fruit in bad_fruits:
Â  Â  Â  Â  Â  Â  Â  Â  center_x, center_y = fruit['center']
Â  Â  Â  Â  Â  Â  Â  Â  depth = fruit['depth']
Â  Â  Â  Â  Â  Â  Â  Â  x, y, w, h = fruit['bbox']
Â  Â  Â  Â  Â  Â  Â  Â  contour = fruit['contour']
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Compute 3D position in 'camera_link' frame
Â  Â  Â  Â  Â  Â  Â  Â  pos_x_cam, pos_y_cam, pos_z_cam = self.compute_3d_position(center_x, center_y, depth)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Match to existing tracked fruit or assign new ID
Â  Â  Â  Â  Â  Â  Â  Â  matched_fruit_id = self.match_fruit_to_tracked(center_x, center_y, depth)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if matched_fruit_id is None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Check all currently tracked AND locked IDs
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  all_ids = list(self.fruit_detection_confidence.keys()) + list(self.locked_fruit_tfs.keys())
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Filter out duplicates and find the maximum ID
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  unique_ids = [int(i) for i in set(all_ids)]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Assign the next available ID
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  new_fruit_id = max(unique_ids) + 1 if unique_ids else 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  matched_fruit_id = new_fruit_id
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.fruit_detection_confidence[matched_fruit_id] = 0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.fruit_detection_positions[matched_fruit_id] = []
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  detected_fruit_ids_in_frame.add(matched_fruit_id)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Store position history for stability tracking (only for pre-locked fruits)
Â  Â  Â  Â  Â  Â  Â  Â  if matched_fruit_id not in self.locked_fruit_tfs:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.fruit_detection_positions[matched_fruit_id].append((pos_x_cam, pos_y_cam, pos_z_cam))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(self.fruit_detection_positions[matched_fruit_id]) > 10:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.fruit_detection_positions[matched_fruit_id].pop(0)Â  # Keep last 10 positions
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Visualization
Â  Â  Â  Â  Â  Â  Â  Â  frame_name = f'{TEAM_ID}_bad_fruit_{matched_fruit_id}'
Â  Â  Â  Â  Â  Â  Â  Â  cv2.drawContours(display_image, [contour], -1, (0, 255, 255), 2)
Â  Â  Â  Â  Â  Â  Â  Â  cv2.rectangle(display_image, (x, y), (x + w, y + h), (0, 255, 0), 2)
Â  Â  Â  Â  Â  Â  Â  Â  cv2.circle(display_image, (center_x, center_y), 5, (0, 0, 255), -1)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Show if locked or detecting
Â  Â  Â  Â  Â  Â  Â  Â  if matched_fruit_id in self.locked_fruit_tfs:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  label = f'bad fruit {matched_fruit_id} (LOCKED)'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  color = (0, 255, 255)Â  # Yellow for locked
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  conf_count = self.fruit_detection_confidence.get(matched_fruit_id, 0)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  label = f'bad fruit {matched_fruit_id} ({conf_count}/{CONFIDENCE_THRESHOLD})'
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  color = (0, 255, 0)Â  # Green for detecting
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  cv2.putText(display_image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Locking Logic
Â  Â  Â  Â  Â  Â  Â  Â  if matched_fruit_id not in self.locked_fruit_tfs:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.fruit_detection_confidence[matched_fruit_id] = min(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  CONFIDENCE_THRESHOLD,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.fruit_detection_confidence[matched_fruit_id] + 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if self.fruit_detection_confidence[matched_fruit_id] >= CONFIDENCE_THRESHOLD:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.lock_fruit_tf(matched_fruit_id, pos_x_cam, pos_y_cam, pos_z_cam)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Reset confidence for non-detected, unlocked fruits
Â  Â  Â  Â  Â  Â  for fruit_id in list(self.fruit_detection_confidence.keys()):
Â  Â  Â  Â  Â  Â  Â  Â  if fruit_id not in self.locked_fruit_tfs and fruit_id not in detected_fruit_ids_in_frame:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Decay confidence faster if fruit is missed
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  self.fruit_detection_confidence[fruit_id] = max(0, self.fruit_detection_confidence[fruit_id] - 2)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if self.fruit_detection_confidence[fruit_id] == 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Remove tracking if confidence drops to 0
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  del self.fruit_detection_confidence[fruit_id]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if fruit_id in self.fruit_detection_positions:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  del self.fruit_detection_positions[fruit_id]
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Continuous Broadcasting of Locked Fruit TFs
Â  Â  Â  Â  for fruit_id, tf_msg in self.locked_fruit_tfs.items():
Â  Â  Â  Â  Â  Â  if tf_msg is not None:
Â  Â  Â  Â  Â  Â  Â  Â  tf_msg.header.stamp = current_stamp
Â  Â  Â  Â  Â  Â  Â  Â  self.tf_broadcaster.sendTransform(tf_msg)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- 3. Display Image ---
Â  Â  Â  Â  if SHOW_IMAGE and display_image is not None:
Â  Â  Â  Â  Â  Â  cv2.imshow('Vision and TF Publisher', display_image)
Â  Â  Â  Â  Â  Â  cv2.waitKey(1)

# --- Main Execution ---
def main(args=None):
Â  Â  rclpy.init(args=args)
Â  Â  node = VisionAndTFPublisher()
Â  Â  try:
Â  Â  Â  Â  rclpy.spin(node)
Â  Â  except KeyboardInterrupt:
Â  Â  Â  Â  pass
Â  Â  finally:
Â  Â  Â  Â  if rclpy.ok():
Â  Â  Â  Â  Â  Â  node.destroy_node()
Â  Â  Â  Â  Â  Â  rclpy.shutdown()
Â  Â  Â  Â  if SHOW_IMAGE:
Â  Â  Â  Â  Â  Â  cv2.destroyAllWindows()

if __name__ == '__main__':
Â  Â  main()


